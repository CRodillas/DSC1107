---
title: "DSC1107_SA1_Rodillas"
author: "Rodillas"
date: "2025-03-19"
output: github_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(caret)
library(glmnet)
```


## Unit 1: R for Data Mining

### 1. Intro to Modern Data Mining

```{r}
churn_df <- read.csv("E:/Downloads/customer_churn.csv")

head(churn_df)
```
This is the first 6 rows of the data set. The customer_churn.csv contains the customers demographics such has customer ID, gender, status (Senior Citizen, Availability of Partner & Dependents) as well as internet contract details such as Tenure, Phone and Internet Service, Contract, and Charges, as well as Churn.

```{r}
dim(churn_df)
```

Listing all columns, the dataset has 12 columns and 10000 rows, accounting for exactly 10000 of the customers.

```{r}
str(churn_df)
```

```{r}
colSums(is.na(churn_df)) 
```

There are no null values for each column, but this could be accounted for by its unfit data types.


```{r}
summary(churn_df)
```

Looking at the overview of the dataset given, it provides sophisticated data of a  large phone and internet service business that includes the details of every customers under the said provider. Data Mining provides a much needed dimension to the business as it procures actions that greatly improves the business. First, data mining can help identify patterns and trends among pre-existing customers, and factors that may help affect these patterns. It opens up a new possibility to data prediction based on the likelihood of a customer churning based on the factors stated. 

In addition, data mining also provides the business an overview of both their target market, and their service improvement. Allowing for a much more systematic understanding and a visualization of the growth, or even the decline, of business.

### 2. Data Visualization

```{r}
churn_contract <- ggplot(data = churn_df, mapping = aes(Contract, fill = Churn)) +
  geom_bar() + 
  geom_text(
    stat = "count",
    aes(label = scales::percent(..count.. / tapply(..count.., ..x.., sum)[..x..]))
  ) +
  labs(
    title = "Churn Rate by Contract Type", 
    x = "Contract Type", 
    y = "Number of Customers") 
churn_contract
  

```

##### Figure 1. Churn Rate of the Customers Based on the Contract Type

Above is a modified bar graph that allows us to check the proportionality of churn rate by contract. Looking at the figure, Month-to-Month holds most of the subscription fit for customers wanting to try the provided service on a short-term basis. It holds 27.54% Churn Rate. On the other hand, both the One Year and the Two Year Types almost hold the same number of customers. Upon reviewing, One Year holds the lowest percentage of customers cancelling their subscription at 24.7%, while two year holds the highest at 28.00%, meaning that One Year contracts hold the least likely for the customer to churn.

```{r}
churn_mcharges <- ggplot(churn_df, aes(x = MonthlyCharges, fill = Churn)) +
  geom_histogram(position = "identity", 
                 alpha = 0.5, 
                 binwidth = 5) +
  labs(
    title = "Distribution of Monthly Charges by Churn Status", 
    x = "Monthly Charges", 
    y = "Count"
  ) 

churn_mcharges

```



##### Figure 2. Churn Status Histogram based on the Monthly Charges

Figure 2 shows how much the monthly charges affects the churn rate of the customers. Based on the graph, Most of the ranges of customers churning in monthly charges hovers around 100 to 150, having customers 70 per month getting the lowest churn rate to count ratio. On the flipside, charges around the range of 75 to 80 per month provides the most profitable range for the business.


```{r}
tenure_churn_data <- churn_df %>%
  group_by(Tenure, Churn) %>%
  summarise(count = n()) %>%
  group_by(Tenure) %>%
  mutate(rate = count / sum(count)) %>%
  filter(Churn == "Yes")

churn_tenure <- ggplot(tenure_churn_data, aes(x = Tenure, y = rate)) +
  geom_line(color = "red") +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Churn Rate by Tenure",
    x = "Tenure (Months)",
    y = "Churn Rate"
  ) 

churn_tenure
```


##### Figure 3. Churn Rate Line Plot based on Tenure

The line plot above shows the Churn rate of the customers based on the longevity of their tenure in the provider. The plot shows at which months usually customers end their contract. Looking at it, the most critical points of the churn rate sits at around 15 to 17, 50, and at around 63, all of which sits around the 35% churn rate.



### 3. Data Transformation

```{r}
unique(churn_df$SeniorCitizen)
```

Looking at these unique values for senior citizen, it is safe to assume that this was used to identify if the customer belong to this particular age group. For uniformity, we can change 0=no and 1= yes
churn_df$SeniorCitizen <- ifelse(churn_df$SeniorCitizen == 1, "Yes", "No")
```{r}
churn_df$SeniorCitizen <- ifelse(churn_df$SeniorCitizen == 1, "Yes", "No")
```

Converting categorical into factors:
```{r}
churn_df$Gender <- as.factor(churn_df$Gender)
churn_df$Partner <- as.factor(churn_df$Partner)
churn_df$Dependents <- as.factor(churn_df$Dependents)
churn_df$PhoneService <- as.factor(churn_df$PhoneService)
churn_df$InternetService <- as.factor(churn_df$InternetService)
churn_df$Contract <- as.factor(churn_df$Contract)
churn_df$Churn <- as.factor(churn_df$Churn)
```

Stardardizing numerical features:
```{r}
churn_df$MonthlyCharges <- scale(churn_df$MonthlyCharges)
churn_df$TotalCharges <- scale(churn_df$TotalCharges)
churn_df$Tenure <- scale(churn_df$Tenure)
```


### 4. Data Wrangling

Creating a function, I used the IQR to filter unnecessary outliers: 
```{r}
filter_outliers <- function(data, cols) {
  for (col in cols) {
    Q1 <- quantile(data[[col]], 0.25, na.rm = TRUE)
    Q3 <- quantile(data[[col]], 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    
    data <- data %>%
      filter(data[[col]] >= (Q1 - 1.5 * IQR) & data[[col]] <= (Q3 + 1.5 * IQR))
  }
  
  return(data)
}

churn_df <- filter_outliers(churn_df, c("MonthlyCharges", "TotalCharges"))
```

Creating new Dervied Variables:

I added two columns, the total contract value which provides the total number a contract has provided, as well as tenure group to easily filter each groups
```{r}
churn_df <- churn_df %>%
  mutate(Total_Contract_Value = MonthlyCharges * Tenure)

churn_df <- churn_df %>%
  mutate(Tenure_Group = case_when(
    Tenure <= 12 ~ "New",
    Tenure <= 36 ~ "Intermediate",
    TRUE ~ "Loyal"
  ))

```

This is the new head of the data set with 2 new additional variables:

```{r}
head(churn_df)
```

## Unit 2: Tuning Predictive Models

### 6. Model Complexity

```{r}
set.seed(777)

trIndex <-createDataPartition(churn_df$Churn,
                              p = 0.8,
                              list = FALSE)
train_df <- churn_df[trIndex,]
test_df <- churn_df[-trIndex,]

```


```{r}
test_df$TotalCharges[is.na(test_df$TotalCharges)] <- median(test_df$TotalCharges, na.rm=TRUE)

#decision tree
DT_churn <- rpart(Churn ~ Tenure + MonthlyCharges + TotalCharges + Gender + Partner + Dependents + PhoneService + InternetService + Contract, 
                             data = test_df, 
                             method = "class", 
                             control = rpart.control(cp = 0, maxdepth = 5))

summary(DT_churn)
```


```{r}
#logistic
LRM_churn <- glm(Churn ~ Tenure + MonthlyCharges + TotalCharges + Gender + Partner + Dependents + PhoneService + InternetService + Contract, 
                      data = test_df, 
                      family = binomial)

summary(LRM_churn)
```


```{r}
DT_pred <- predict(DT_churn, test_df, type = "class")
confusionMatrix(DT_pred, test_df$Churn)

log_pred <- predict(LRM_churn, test_df, type = "response")
log_class <- ifelse(log_pred > 0.5, "Yes", "No")
confusionMatrix(as.factor(log_class), test_df$Churn)

```

```{r}
rpart.plot(DT_churn)
```

##### Figure 4. Decision Tree Plot 

**Complexity and Trade Offs**

Both decision trees and logistic regression are few of the most popular models for classification, but differ in what they offer in complexity and trade offs. Decision trees recursively splits the data based on the values, showing relationships between nonlinear. The only problem with decision trees is its tendency of overfitting due to its nature of trying to accurately split the data smaller and more specific based on the vakyes. Because it improves the accuracy of the training data, it increases its complexity, and in return making it more susceptible to noise and variations. Reasons such as high variance can drastically change the tree structure.

On the other hand, logistic regression assume a linear relationship between the predictors and the target variable. Because it only assumes linear relationship, its complexity is less and more simpler. Logistic regression tends to generalize better and unlike trees, is less prone to overfitting.

While decision trees are better suited for handling nonlinearity and feature interactions, logistic regression is more suitable when the relationship is straightforward and when interpretability of coefficients is important. If the goal is to understand the relationship between variables and predict probabilities, logistic regression is preferable. However, if the focus is on capturing complex decision boundaries or interpreting decision rules, decision trees are more effective.

### 7. Bias-Variance Trade-Off

The Bias Variance Trade off is a usual dilemma which describes the balance between the model's capability to capture complex patterns in variance, and show meaningful generalization of data in bias.

Bias refers to the error when the model is too simplistic, making the assumptions be simplistic as well. A high-bias tends to underfit the data, failing to capture the patterns and the relationship.On the other side, we have Variance, which refers to the error is too sensitive to noise in training. It tends to overfit the model, capturing noise on top of the patterns. 

The trade-off between thse two is to find a balance that minimizes the total error on training and testing data. Increasing model complexity typically reduces bias but increases variance, while simplifying a model reduces variance but increases bias.

In the context of the decision tree and logistic regression models trained, this trade-off plays a key role in determining their predictive performance. Decision trees are low-bias but high-variance models. When the tree is deep and complex , the model can perfectly fit the training data, reducing bias but increasing variance. This means the model will have high accuracy on the training set but may fail to generalize to new data because it captures noise and specific details that are not present in future samples. 

Logistic regression, on the other hand, is a high-bias but low-variance model. Since it assumes a linear relationship between predictors and the target variable, it may underfit complex data patterns, leading to high bias. 

### 8. Cross-Validation

```{r}
control <- trainControl(method = "cv", number = 10)

logit_cv <- train(Churn ~ Tenure + MonthlyCharges + TotalCharges + InternetService + Contract, data = train_df,
                  method = "glm",
                  family = binomial, 
                  trControl = control)

print(logit_cv)
```

```{r}
# for dec tree w cross validation
dt_cv <- train(Churn ~ Tenure + MonthlyCharges + TotalCharges + Gender + Partner + Dependents + PhoneService + InternetService + Contract, 
               data = train_df, 
               method = "rpart", 
               trControl = control)

print(dt_cv)
print(confusionMatrix(dt_cv))

```

```{r}

#for logis regress
log_cv <- train(Churn ~ Tenure + MonthlyCharges + TotalCharges + Gender + Partner + Dependents + PhoneService + InternetService + Contract, 
                data = train_df, 
                method = "glm", 
                family = "binomial",
                trControl = control)

print(log_cv)
print(confusionMatrix(log_cv))
```
```{r}
dt_conf_matrix <- matrix(c(65.1, 24.4, 7.8, 2.7), nrow = 2, byrow = TRUE)
log_conf_matrix <- matrix(c(72.9, 27.1, 0, 0), nrow = 2, byrow = TRUE)

calculate_overall_metrics <- function(conf_matrix) {
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
  
  precision <- ifelse(sum(conf_matrix[2, ]) == 0, 0, conf_matrix[2, 2] / sum(conf_matrix[2, ]))
  recall <- ifelse(sum(conf_matrix[, 2]) == 0, 0, conf_matrix[2, 2] / sum(conf_matrix[, 2]))
  f1 <- ifelse(precision + recall == 0, 0, 2 * (precision * recall) / (precision + recall))
  
  return(c(accuracy, precision, recall, f1))
}

# Get results
dt_results <- calculate_overall_metrics(dt_conf_matrix)
log_results <- calculate_overall_metrics(log_conf_matrix)

results_table <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score"),
  Decision_Tree = dt_results,
  Logistic_Regression = log_results
)

print(results_table)

```

Interpretation and Comparison:
for the Decision Tree's accuracy, the model correctly predicted the class labels around 67.72% of the time during the cross-validation. Compared to the Logistic Regression with 72.9%, this suggests that the latter model is better at predicting churn and non-churn cases. But when it comes to precision, recall, and F-score, Logistic failed to produce, predict, and identify churn cases. On the other hand, although lacking, the decision tree still provides a 26% prediction rate when predicting if the customer will churn, but is low on both recall and f1-score.

All in all, the Decision Tree model demonstrates a modest balance between precision and recall, but the low recall suggests that it struggles to capture all instances of churn. The Logistic Regression model achieves higher overall accuracy, but at the cost of failing to identify churn cases, as reflected in the 0% precision and recall. This suggests that the Logistic Regression model is likely biased toward predicting non-churn cases, which could be due to class imbalance or inadequate feature representation.

## Unit 3: Regression-Based Methods

### 10. Logistic Regression
```{r}
logistic_churn <- glm(Churn ~ Tenure + MonthlyCharges + TotalCharges, 
                      data = train_df, 
                      family = binomial)

summary(logistic_churn)

```

The summary above shows the model's logistic regression. First, looking at the p-value, none of the independent variables have a p-value<0.05, meaning that none of which are statistically significant. The High AIC of 9288.7 suggests that the logistic regression model does not explain much variance on churn.

### 11. Regression in High Dimensions

High dimensional regression present several challenges that generally hinder the performance and interpretation. One key issue is the curse of **dimensionality**, where increasing the number of predictors makes the data sparse, making it harder for models to identify meaningful patterns and increasing the computational complexity.

Another is **multicollinearity**, where highly correlated predictors make it difficult for the model to separate their individual effects, leading to unstable and unreliable coefficient estimates. 

High-dimensional models are also more prone to **overfitting**, as they tend to capture noise rather than genuine patterns, reducing their ability to generalize to new data. Additionally, the increased complexity of these models reduces interpretability, making it harder to understand the relationships between predictors and the outcome.
```{r}
numeric_features <- train_df[, c("Tenure", "MonthlyCharges", "TotalCharges")]
numeric_features_scaled <- scale(numeric_features)
pca_model <- prcomp(numeric_features_scaled, center = TRUE, scale. = TRUE)
pca_model
```

### 12. Ridge Regression

```{r}
x <- model.matrix(Churn ~ Tenure + MonthlyCharges + TotalCharges + Gender + Partner + Dependents + PhoneService + InternetService + Contract, data = train_df)[, -1]
y <- train_df$Churn

y <- ifelse(y == "Yes", 1, 0)

set.seed(777)
cv_ridge <- cv.glmnet(x, y, alpha = 0)

optimal_lambda <- cv_ridge$lambda.min
cat("Optimal lambda:", optimal_lambda, "\n")

ridge_model <- glmnet(x, y, alpha = 0, lambda = optimal_lambda)

coef(ridge_model)

colnames(train_df)


```

After implementing Ridge Regression model by training using Churn as the target variable, the optimal lambda through cross-validation was 1.579187, which balances the bias-variance tradeoff by lowering the coefficients. 


### 13. Lasso Regression


```{r}
x <- as.matrix(train_df[, c("Tenure", "MonthlyCharges", "TotalCharges", 
                            "Gender", "Partner", "Dependents", 
                            "PhoneService", "InternetService", 
                            "Contract")])
y_lasso <- train_df$Churn

set.seed(777)
lasso_cv <- cv.glmnet(x, y, alpha = 1, family = "binomial")

optimal_lambda <- lasso_cv$lambda.min
print(optimal_lambda)

lasso_model <- glmnet(x, y, alpha = 1, lambda = optimal_lambda, family = "binomial")

coef(lasso_model)

```

Lasso regression finds the best feature for the predictors and as seen on the results, they have shrank to exactly 0. This indicates that they are all not strongly associated with the target variable Churn.